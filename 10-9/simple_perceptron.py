"""
Simple Perceptron Implementation (Without Learning Rate)
ç°¡å–®æ„ŸçŸ¥æ©Ÿå¯¦ä½œï¼ˆç„¡å­¸ç¿’ç‡ç‰ˆæœ¬ï¼‰

Based on Rosenblatt's early formulation of the Perceptron Convergence Theorem.
åŸºæ–¼ Rosenblatt æ—©æœŸæ„ŸçŸ¥æ©Ÿæ”¶æ–‚å®šç†çš„è¡¨è¿°ã€‚

Author: Teaching Assistant
Date: 2025
"""

import numpy as np
import matplotlib.pyplot as plt
from typing import List, Tuple, Optional

class SimplePerceptron:
    """
    Simple Perceptron without learning rate (Rosenblatt's early form)
    ç°¡å–®æ„ŸçŸ¥æ©Ÿï¼ˆç„¡å­¸ç¿’ç‡ï¼ŒRosenblatt æ—©æœŸå½¢å¼ï¼‰
    
    This implementation follows the classical error-correction rule:
    - If prediction is wrong: w = w + x (for positive class) or w = w - x (for negative class)
    - If prediction is correct: no update
    
    é€™å€‹å¯¦ä½œéµå¾ªç¶“å…¸çš„éŒ¯èª¤ä¿®æ­£è¦å‰‡ï¼š
    - å¦‚æœé æ¸¬éŒ¯èª¤ï¼šw = w + xï¼ˆæ­£é¡ï¼‰æˆ– w = w - xï¼ˆè² é¡ï¼‰
    - å¦‚æœé æ¸¬æ­£ç¢ºï¼šä¸æ›´æ–°
    """
    
    def __init__(self, threshold: float = 0.0):
        """
        Initialize the perceptron
        åˆå§‹åŒ–æ„ŸçŸ¥æ©Ÿ
        
        Args:
            threshold: Decision threshold Î¸ (default: 0.0)
                      æ±ºç­–é–¾å€¼ Î¸ï¼ˆé è¨­ï¼š0.0ï¼‰
        """
        self.weights = None
        self.threshold = threshold
        self.training_history = []
        self.converged = False
        self.n_updates = 0
    
    def predict(self, x: np.ndarray) -> int:
        """
        Make prediction for input x
        å°è¼¸å…¥ x é€²è¡Œé æ¸¬
        
        Args:
            x: Input vector (numpy array)
               è¼¸å…¥å‘é‡
        
        Returns:
            +1 if wÂ·x > Î¸, -1 otherwise
            å¦‚æœ wÂ·x > Î¸ å‰‡è¿”å› +1ï¼Œå¦å‰‡è¿”å› -1
        """
        if self.weights is None:
            raise ValueError("Model not trained yet! æ¨¡å‹å°šæœªè¨“ç·´ï¼")
        
        activation = np.dot(self.weights, x)
        return 1 if activation > self.threshold else -1
    
    def fit(self, X: np.ndarray, y: np.ndarray, max_epochs: int = 1000) -> bool:
        """
        Train the perceptron using error-correction rule
        ä½¿ç”¨éŒ¯èª¤ä¿®æ­£è¦å‰‡è¨“ç·´æ„ŸçŸ¥æ©Ÿ
        
        Args:
            X: Training data (n_samples, n_features)
               è¨“ç·´è³‡æ–™
            y: Training labels (+1 or -1)
               è¨“ç·´æ¨™ç±¤ï¼ˆ+1 æˆ– -1ï¼‰
            max_epochs: Maximum number of epochs
                       æœ€å¤§è¨“ç·´è¼ªæ•¸
        
        Returns:
            True if converged, False if max_epochs reached
            å¦‚æœæ”¶æ–‚å‰‡è¿”å› Trueï¼Œé”åˆ°æœ€å¤§è¼ªæ•¸å‰‡è¿”å› False
        """
        n_samples, n_features = X.shape
        
        # Initialize weights to zero (classical approach)
        # å°‡æ¬Šé‡åˆå§‹åŒ–ç‚ºé›¶ï¼ˆç¶“å…¸æ–¹æ³•ï¼‰
        self.weights = np.zeros(n_features)
        self.training_history = []
        self.n_updates = 0
        
        print(f"é–‹å§‹è¨“ç·´æ„ŸçŸ¥æ©Ÿ...")
        print(f"è³‡æ–™é»æ•¸é‡: {n_samples}, ç‰¹å¾µç¶­åº¦: {n_features}")
        print(f"é–¾å€¼ Î¸ = {self.threshold}")
        print("-" * 50)
        
        for epoch in range(max_epochs):
            n_errors = 0
            epoch_updates = []
            
            # Go through all training samples
            # éæ­·æ‰€æœ‰è¨“ç·´æ¨£æœ¬
            for i in range(n_samples):
                x_i = X[i]
                y_i = y[i]
                
                # Make prediction
                # é€²è¡Œé æ¸¬
                prediction = self.predict(x_i)
                
                # Check if prediction is wrong
                # æª¢æŸ¥é æ¸¬æ˜¯å¦éŒ¯èª¤
                if prediction != y_i:
                    # Error correction update (no learning rate!)
                    # éŒ¯èª¤ä¿®æ­£æ›´æ–°ï¼ˆç„¡å­¸ç¿’ç‡ï¼ï¼‰
                    if y_i == 1:
                        self.weights += x_i  # w = w + x
                    else:
                        self.weights -= x_i  # w = w - x
                    
                    self.n_updates += 1
                    n_errors += 1
                    
                    epoch_updates.append({
                        'sample_idx': i,
                        'input': x_i.copy(),
                        'true_label': y_i,
                        'prediction': prediction,
                        'weights_after': self.weights.copy()
                    })
            
            # Record training history
            # è¨˜éŒ„è¨“ç·´æ­·å²
            self.training_history.append({
                'epoch': epoch + 1,
                'n_errors': n_errors,
                'weights': self.weights.copy(),
                'updates': epoch_updates
            })
            
            print(f"Epoch {epoch + 1:3d}: {n_errors} å€‹éŒ¯èª¤, æ¬Šé‡ = {self.weights}")
            
            # Check convergence
            # æª¢æŸ¥æ”¶æ–‚
            if n_errors == 0:
                self.converged = True
                print(f"\nğŸ‰ æ„ŸçŸ¥æ©Ÿå·²æ”¶æ–‚ï¼")
                print(f"ç¸½æ›´æ–°æ¬¡æ•¸: {self.n_updates}")
                print(f"æœ€çµ‚æ¬Šé‡: {self.weights}")
                return True
        
        print(f"\nâš ï¸  é”åˆ°æœ€å¤§è¨“ç·´è¼ªæ•¸ ({max_epochs})ï¼Œæœªå®Œå…¨æ”¶æ–‚")
        return False
    
    def plot_decision_boundary(self, X: np.ndarray, y: np.ndarray, 
                             title: str = "æ„ŸçŸ¥æ©Ÿæ±ºç­–é‚Šç•Œ"):
        """
        Plot decision boundary (only works for 2D data)
        ç¹ªè£½æ±ºç­–é‚Šç•Œï¼ˆåƒ…é©ç”¨æ–¼ 2D è³‡æ–™ï¼‰
        """
        if X.shape[1] != 2:
            print("åªèƒ½ç¹ªè£½ 2D è³‡æ–™çš„æ±ºç­–é‚Šç•Œ")
            return
        
        plt.figure(figsize=(10, 8))
        
        # Plot data points
        # ç¹ªè£½è³‡æ–™é»
        positive_mask = y == 1
        negative_mask = y == -1
        
        plt.scatter(X[positive_mask, 0], X[positive_mask, 1], 
                   c='red', marker='o', s=100, label='æ­£é¡ (+1)', alpha=0.7)
        plt.scatter(X[negative_mask, 0], X[negative_mask, 1], 
                   c='blue', marker='s', s=100, label='è² é¡ (-1)', alpha=0.7)
        
        # Plot decision boundary
        # ç¹ªè£½æ±ºç­–é‚Šç•Œ
        if self.weights is not None:
            w1, w2 = self.weights
            
            # Decision boundary: w1*x1 + w2*x2 = Î¸
            # æ±ºç­–é‚Šç•Œï¼šw1*x1 + w2*x2 = Î¸
            x_min, x_max = plt.xlim()
            
            if abs(w2) > 1e-10:  # Avoid division by zero
                x_boundary = np.linspace(x_min, x_max, 100)
                y_boundary = (self.threshold - w1 * x_boundary) / w2
                plt.plot(x_boundary, y_boundary, 'g-', linewidth=2, 
                        label=f'æ±ºç­–é‚Šç•Œ: {w1:.2f}xâ‚ + {w2:.2f}xâ‚‚ = {self.threshold}')
            else:
                # Vertical line
                x_boundary = self.threshold / w1 if abs(w1) > 1e-10 else 0
                plt.axvline(x=x_boundary, color='g', linewidth=2, 
                           label=f'æ±ºç­–é‚Šç•Œ: xâ‚ = {x_boundary:.2f}')
        
        plt.xlabel('ç‰¹å¾µ 1 (xâ‚)')
        plt.ylabel('ç‰¹å¾µ 2 (xâ‚‚)')
        plt.title(title)
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.axis('equal')
        plt.show()
    
    def get_training_summary(self) -> dict:
        """
        Get summary of training process
        ç²å–è¨“ç·´éç¨‹æ‘˜è¦
        """
        return {
            'converged': self.converged,
            'n_epochs': len(self.training_history),
            'n_updates': self.n_updates,
            'final_weights': self.weights.copy() if self.weights is not None else None,
            'threshold': self.threshold
        }


def create_linearly_separable_data(n_samples: int = 20, 
                                 random_state: Optional[int] = 42) -> Tuple[np.ndarray, np.ndarray]:
    """
    Create a simple linearly separable 2D dataset
    å‰µå»ºç°¡å–®çš„ç·šæ€§å¯åˆ† 2D è³‡æ–™é›†
    
    Args:
        n_samples: Number of samples per class
                  æ¯é¡çš„æ¨£æœ¬æ•¸é‡
        random_state: Random seed for reproducibility
                     éš¨æ©Ÿç¨®å­
    
    Returns:
        X: Feature matrix (2*n_samples, 2)
        y: Labels (2*n_samples,)
    """
    if random_state is not None:
        np.random.seed(random_state)
    
    # Generate positive class samples (upper right)
    # ç”Ÿæˆæ­£é¡æ¨£æœ¬ï¼ˆå³ä¸Šæ–¹ï¼‰
    X_pos = np.random.randn(n_samples, 2) + [2, 2]
    y_pos = np.ones(n_samples)
    
    # Generate negative class samples (lower left)
    # ç”Ÿæˆè² é¡æ¨£æœ¬ï¼ˆå·¦ä¸‹æ–¹ï¼‰
    X_neg = np.random.randn(n_samples, 2) + [-2, -2]
    y_neg = -np.ones(n_samples)
    
    # Combine data
    # åˆä½µè³‡æ–™
    X = np.vstack([X_pos, X_neg])
    y = np.hstack([y_pos, y_neg])
    
    # Shuffle data
    # æ‰“äº‚è³‡æ–™
    indices = np.random.permutation(len(X))
    X = X[indices]
    y = y[indices]
    
    return X, y


def demonstrate_perceptron():
    """
    Demonstrate the perceptron algorithm
    æ¼”ç¤ºæ„ŸçŸ¥æ©Ÿæ¼”ç®—æ³•
    """
    print("=" * 60)
    print("æ„ŸçŸ¥æ©Ÿæ¼”ç®—æ³•æ¼”ç¤º (Perceptron Algorithm Demonstration)")
    print("=" * 60)
    
    # Create dataset
    # å‰µå»ºè³‡æ–™é›†
    print("\n1. å‰µå»ºç·šæ€§å¯åˆ†è³‡æ–™é›†...")
    X, y = create_linearly_separable_data(n_samples=10, random_state=42)
    print(f"è³‡æ–™å½¢ç‹€: {X.shape}")
    print(f"æ¨™ç±¤: {np.unique(y, return_counts=True)}")
    
    # Initialize and train perceptron
    # åˆå§‹åŒ–ä¸¦è¨“ç·´æ„ŸçŸ¥æ©Ÿ
    print("\n2. åˆå§‹åŒ–æ„ŸçŸ¥æ©Ÿ...")
    perceptron = SimplePerceptron(threshold=0.0)
    
    print("\n3. é–‹å§‹è¨“ç·´...")
    success = perceptron.fit(X, y, max_epochs=100)
    
    # Show results
    # é¡¯ç¤ºçµæœ
    print("\n4. è¨“ç·´çµæœ:")
    summary = perceptron.get_training_summary()
    for key, value in summary.items():
        print(f"   {key}: {value}")
    
    # Test predictions
    # æ¸¬è©¦é æ¸¬
    print("\n5. æ¸¬è©¦é æ¸¬:")
    for i in range(min(5, len(X))):
        pred = perceptron.predict(X[i])
        print(f"   æ¨£æœ¬ {i}: çœŸå¯¦={int(y[i]):2d}, é æ¸¬={pred:2d}, "
              f"è¼¸å…¥={X[i]}, æ­£ç¢º={pred == y[i]}")
    
    # Plot results
    # ç¹ªè£½çµæœ
    print("\n6. ç¹ªè£½æ±ºç­–é‚Šç•Œ...")
    perceptron.plot_decision_boundary(X, y, "æ„ŸçŸ¥æ©Ÿå­¸ç¿’çµæœ")
    
    return perceptron, X, y


if __name__ == "__main__":
    # Run demonstration
    # åŸ·è¡Œæ¼”ç¤º
    perceptron, X, y = demonstrate_perceptron()
